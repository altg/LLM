{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/altaf/Projects/LLM/Llama/llama-models/llama-2-7b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 5185.72 MB (+ 1026.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=\"/Users/altaf/Projects/LLM/Llama/llama-models/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_message = \"\"\"\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"When was the moon landing?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_message = \"\"\"\n",
    "You are a helpful, respectful and honest text summarisation assistant, please summarise the following text in 50 words or less.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"Last year COVID-19 kept us apart. This year we are finally together again. \n",
    "Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \n",
    "With a duty to one another to the American people to the Constitution. \n",
    "And with an unwavering resolve that freedom will always triumph over tyranny. \n",
    "Six days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \n",
    "He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \n",
    "He met the Ukrainian people. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "[INST]\n",
      "<<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "When was the moon landing?\n",
      "[/INST]\n",
      "\n",
      "Thank you for your kind words! I'm here to help you with any questions you may have, while ensuring that my responses are socially unbiased and positive in nature.\n",
      "To answer your question, the moon landing is a complex and fascinating topic. However, I must inform you that there was no moon landing as the moon does not support human life or space travel. The moon is a celestial body that orbits the Earth, and it is not possible for humans to land on its surface.\n",
      "I understand that this may be disappointing, but I hope you can appreciate the beauty and wonder of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 16202.26 ms\n",
      "llama_print_timings:      sample time =   157.07 ms /   128 runs   (    1.23 ms per token,   814.93 tokens per second)\n",
      "llama_print_timings: prompt eval time = 16202.14 ms /   148 tokens (  109.47 ms per token,     9.13 tokens per second)\n",
      "llama_print_timings:        eval time = 31142.33 ms /   127 runs   (  245.22 ms per token,     4.08 tokens per second)\n",
      "llama_print_timings:       total time = 47930.06 ms\n"
     ]
    }
   ],
   "source": [
    "msg = f\"\"\" \n",
    "[INST]\n",
    "<<SYS>>{sys_message}<</SYS>>\n",
    "{prompt}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "print(msg)\n",
    "\n",
    "response = llm( msg )\n",
    "\n",
    "print(response['choices'][0]['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " сайт онлайн бесплатно, в дополнение к тому, как мы делаемся всегда на увеличение доходов наших клиентов, мы делаем посильное усилие для того чтобы защитить их, что принесет вам невероятный уровень безопасности и ответственность.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 10054.23 ms\n",
      "llama_print_timings:      sample time =    91.78 ms /    80 runs   (    1.15 ms per token,   871.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1113.21 ms /     6 tokens (  185.54 ms per token,     5.39 tokens per second)\n",
      "llama_print_timings:        eval time = 19290.72 ms /    79 runs   (  244.19 ms per token,     4.10 tokens per second)\n",
      "llama_print_timings:       total time = 20743.85 ms\n"
     ]
    }
   ],
   "source": [
    "response = llm( \"When was the moon landing?\")\n",
    "\n",
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
