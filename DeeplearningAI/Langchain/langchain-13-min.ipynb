{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rabbitmetrics/langchain-13-min/blob/main/notebooks/langchain-13-min.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "50dvxjqCFmhF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load environment variables\n",
        "\n",
        "from dotenv import load_dotenv,find_dotenv\n",
        "load_dotenv(find_dotenv())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "C7RnyUOCJWmk"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/altaf/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n\\nLarge language models are neural networks that use large amounts of text data to learn the statistical properties of language and generate text.'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run basic query with OpenAI wrapper\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(model_name=\"text-davinci-003\")\n",
        "llm(\"explain large language models in one sentence\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JtHgQ5XpJmgi"
      },
      "outputs": [],
      "source": [
        "# import schema for chat messages and ChatOpenAI in order to query chatmodels GPT-3.5-turbo or GPT-4\n",
        "\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "from langchain.chat_models import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yrfYfKfdJyyF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sure! Here's an example of a Python script that trains a neural network on simulated data using the Keras library:\n",
            "\n",
            "```python\n",
            "import numpy as np\n",
            "from keras.models import Sequential\n",
            "from keras.layers import Dense\n",
            "\n",
            "# Generate simulated data\n",
            "np.random.seed(0)\n",
            "X = np.random.rand(100, 2)\n",
            "y = np.random.randint(2, size=100)\n",
            "\n",
            "# Define the neural network model\n",
            "model = Sequential()\n",
            "model.add(Dense(4, input_dim=2, activation='relu'))\n",
            "model.add(Dense(1, activation='sigmoid'))\n",
            "\n",
            "# Compile the model\n",
            "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
            "\n",
            "# Train the model\n",
            "model.fit(X, y, epochs=10, batch_size=10)\n",
            "\n",
            "# Evaluate the model\n",
            "loss, accuracy = model.evaluate(X, y)\n",
            "print('Loss:', loss)\n",
            "print('Accuracy:', accuracy)\n",
            "```\n",
            "\n",
            "In this script, we first generate simulated data using `numpy.random.rand()` and `numpy.random.randint()`. We then define a neural network model using the `Sequential` class from Keras and add two dense layers using the `Dense` class. The first layer has 4 units and uses the ReLU activation function, while the second layer has 1 unit and uses the sigmoid activation function.\n",
            "\n",
            "Next, we compile the model using the `compile()` method, specifying the loss function, optimizer, and metrics to track during training. In this case, we use binary cross-entropy as the loss function, Adam as the optimizer, and accuracy as the metric.\n",
            "\n",
            "We then train the model using the `fit()` method, passing in the input data `X` and target labels `y`, as well as the number of epochs and batch size.\n",
            "\n",
            "Finally, we evaluate the trained model using the `evaluate()` method, which returns the loss and accuracy on the provided data.\n"
          ]
        }
      ],
      "source": [
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\",temperature=0.3)\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are an expert data scientist\"),\n",
        "    HumanMessage(content=\"Write a Python script that trains a neural network on simulated data \")\n",
        "]\n",
        "response=chat(messages)\n",
        "\n",
        "print(response.content,end='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2grf7I8AJ_hK"
      },
      "outputs": [],
      "source": [
        "# Import prompt and define PromptTemplate\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "You are an expert data scientist with an expertise in building deep learning models. \n",
        "Explain the concept of {concept} in a couple of lines\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"concept\"],\n",
        "    template=template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vcz7Q9Y-KFvI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nAn autoencoder is a type of artificial neural network used for unsupervised learning that learns a compressed representation of input data. It is composed of an encoder that maps the input data to a compressed representation and a decoder that reconstructs the input data from the compressed representation.'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run LLM with PromptTemplate\n",
        "\n",
        "llm(prompt.format(concept=\"autoencoder\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dm78i-rUKXIB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "An autoencoder is a type of artificial neural network used to learn efficient data representations (or features) in an unsupervised manner. It works by encoding the input data into a low-dimensional representation, which is then decoded back into the original data space. Autoencoders are used for dimensionality reduction, unsupervised feature learning, and representation learning.\n"
          ]
        }
      ],
      "source": [
        "# Import LLMChain and define chain with language model and prompt as arguments.\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Run the chain only specifying the input variable.\n",
        "print(chain.run(\"autoencoder\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "B6MF4-nMKul3"
      },
      "outputs": [],
      "source": [
        "# Define a second prompt \n",
        "\n",
        "second_prompt = PromptTemplate(\n",
        "    input_variables=[\"ml_concept\"],\n",
        "    template=\"Turn the concept description of {ml_concept} and explain it to me like I'm five in 500 words\",\n",
        ")\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SkJKFyk1K-MO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m\n",
            "An autoencoder is a type of artificial neural network used to learn efficient representations of data, also known as a compressed version of the input data. Autoencoders are used to reduce the dimensionality of data while preserving the important features of the data. Autoencoders are composed of an encoder and a decoder, which work together to learn a representation of the data.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m or less\n",
            "\n",
            "An autoencoder is a cool machine that can help us understand data better. It's like a special computer program that can take a bunch of information and sort it out. It does this by looking at the data and figuring out which parts are most important.\n",
            "\n",
            "Think of it like a game of hide and seek. The autoencoder is like a seeker, trying to find out what's important in the data by looking for patterns and important details. It can then take this important information and hide it away, so that it's easier for us to work with.\n",
            "\n",
            "The autoencoder is made of two parts. The first is called the encoder. This is like the seeking part, it looks at the data and tries to figure out what's important. The second part is called the decoder. This is like the hiding part. It takes the important things the encoder found and stores them away in a special spot, making them easier to find later.\n",
            "\n",
            "This hiding-spot is called a “compressed version” of the data. It's like taking all the important details and squishing them together in a way that takes up less space. It also helps us figure out which parts of the\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " or less\n",
            "\n",
            "An autoencoder is a cool machine that can help us understand data better. It's like a special computer program that can take a bunch of information and sort it out. It does this by looking at the data and figuring out which parts are most important.\n",
            "\n",
            "Think of it like a game of hide and seek. The autoencoder is like a seeker, trying to find out what's important in the data by looking for patterns and important details. It can then take this important information and hide it away, so that it's easier for us to work with.\n",
            "\n",
            "The autoencoder is made of two parts. The first is called the encoder. This is like the seeking part, it looks at the data and tries to figure out what's important. The second part is called the decoder. This is like the hiding part. It takes the important things the encoder found and stores them away in a special spot, making them easier to find later.\n",
            "\n",
            "This hiding-spot is called a “compressed version” of the data. It's like taking all the important details and squishing them together in a way that takes up less space. It also helps us figure out which parts of the\n"
          ]
        }
      ],
      "source": [
        "# Define a sequential chain using the two chains above: the second chain takes the output of the first chain as input\n",
        "\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
        "\n",
        "# Run the chain specifying only the input variable for the first chain.\n",
        "explanation = overall_chain.run(\"autoencoder\")\n",
        "print(explanation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mDDu1B_SLQls"
      },
      "outputs": [],
      "source": [
        "# Import utility for splitting up texts and split up the explanation given above into document chunks\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 100,\n",
        "    chunk_overlap  = 0,\n",
        ")\n",
        "\n",
        "texts = text_splitter.create_documents([explanation])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "F6lfAdeuLhtp"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'or less'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Individual text chunks can be accessed with \"page_content\"\n",
        "\n",
        "texts[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Z5sv4e3tLw2y"
      },
      "outputs": [
        {
          "ename": "ValidationError",
          "evalue": "1 validation error for OpenAIEmbeddings\nmodel_name\n  extra fields not permitted (type=value_error.extra)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[1;32m/Users/altaf/Projects/LLM/DeeplearningAI/Langchain/langchain-13-min.ipynb Cell 13\u001b[0m in \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/altaf/Projects/LLM/DeeplearningAI/Langchain/langchain-13-min.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Import and instantiate OpenAI embeddings\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/altaf/Projects/LLM/DeeplearningAI/Langchain/langchain-13-min.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membeddings\u001b[39;00m \u001b[39mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/altaf/Projects/LLM/DeeplearningAI/Langchain/langchain-13-min.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m embeddings \u001b[39m=\u001b[39m OpenAIEmbeddings(model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mada\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for OpenAIEmbeddings\nmodel_name\n  extra fields not permitted (type=value_error.extra)"
          ]
        }
      ],
      "source": [
        "# Import and instantiate OpenAI embeddings\n",
        "\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model_name=\"ada\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dqzoir4hMlfl"
      },
      "outputs": [],
      "source": [
        "# Turn the first text chunk into a vector with the embedding\n",
        "\n",
        "query_result = embeddings.embed_query(texts[0].page_content)\n",
        "print(query_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaOY5bIZM3Xz"
      },
      "outputs": [],
      "source": [
        "# Import and initialize Pinecone client\n",
        "\n",
        "import os\n",
        "import pinecone\n",
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "\n",
        "pinecone.init(\n",
        "    api_key=os.getenv('PINECONE_API_KEY'),  \n",
        "    environment=os.getenv('PINECONE_ENV')  \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZhSUt3FNBzN"
      },
      "outputs": [],
      "source": [
        "# Upload vectors to Pinecone\n",
        "\n",
        "index_name = \"langchain-quickstart\"\n",
        "search = Pinecone.from_documents(texts, embeddings, index_name=index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCXVuXwPNKcc"
      },
      "outputs": [],
      "source": [
        "# Do a simple vector similarity search\n",
        "\n",
        "query = \"What is magical about an autoencoder?\"\n",
        "result = search.similarity_search(query)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4Ogz8luZNRnJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/altaf/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Import Python REPL tool and instantiate Python agent\n",
        "\n",
        "from langchain.agents.agent_toolkits import create_python_agent\n",
        "from langchain.tools.python.tool import PythonREPLTool\n",
        "from langchain.python import PythonREPL\n",
        "from langchain.llms.openai import OpenAI\n",
        "\n",
        "agent_executor = create_python_agent(\n",
        "    llm=OpenAI(temperature=0, max_tokens=1000),\n",
        "    tool=PythonREPLTool(),\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BVHMDj0sNi09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to calculate the value of the equation for each value of x\n",
            "Action: Python_REPL\n",
            "Action Input: for x in range(5, 11): print(x**3 + 5*x**2 + 3*x + 1)\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m266\n",
            "415\n",
            "610\n",
            "857\n",
            "1162\n",
            "1531\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: 266, 415, 610, 857, 1162, 1531\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'266, 415, 610, 857, 1162, 1531'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Execute the Python agent\n",
        "\n",
        "agent_executor.run(\"What is x**3 + 5*x**2 + 3*x + 1, for the values of x between 5 and 10?\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyONM96f7/m0jUCD9c87+MQy",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
