{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Test Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate,HumanMessagePromptTemplate,PromptTemplate\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your desired data structure.\n",
    "\n",
    "class MultipleChoice(BaseModel):\n",
    "    choice_ans: str = Field(description=\"multiple choice answer\")\n",
    "\n",
    "class Question(BaseModel):\n",
    "    question: str = Field(description=\"the question being asked\")\n",
    "    #answer: str = Field(description=\"answer to the question\")\n",
    "    MultipleChoices: List[MultipleChoice]\n",
    "    correct_ans: int = Field(description=\"the index of the correct answer\")\n",
    "\n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    @validator(\"question\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field\n",
    "\n",
    "\n",
    "class Questions(BaseModel):\n",
    "    Questions: List[Question]\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "question_query = \"Ask me a 3 questions in the specified subject and which would be suitable for students in school level given,\\\n",
    "      and provide 4 multiple choice answers where only 1 is correct. \\\n",
    "        Also provide index of the correct answer.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=Questions)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\nsubject: {subject} , school_level: {level}\\n\",\n",
    "    input_variables=[\"query\", \"subject\", \"level\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "_input = prompt.format_prompt(query=question_query, subject=\"English\" , level=\"Grade 12\")\n",
    "\n",
    "output = llm(_input.to_messages())\n",
    "\n",
    "res = parser.parse(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the definition of a metaphor?\n",
      "A figure of speech that compares two unlike things without using like or as\n",
      "A type of poem that follows a specific rhyme scheme and rhythm\n",
      "The repetition of consonant sounds at the beginning of words\n",
      "A comparison between two unlike things using like or as\n",
      "Correct Answer:  1 : A figure of speech that compares two unlike things without using like or as\n",
      "Which of the following is an example of personification?\n",
      "The stars twinkled in the night sky\n",
      "The wind whispered through the trees\n",
      "She sells seashells by the seashore\n",
      "The waves crashed angrily against the shore\n",
      "Correct Answer:  2 : The wind whispered through the trees\n",
      "What is the purpose of a thesis statement in an essay?\n",
      "To provide a summary of the main points\n",
      "To introduce the topic of the essay\n",
      "To present evidence and examples\n",
      "To express the writer's opinion or argument\n",
      "Correct Answer:  4 : To express the writer's opinion or argument\n"
     ]
    }
   ],
   "source": [
    "for i in res.Questions:\n",
    "    print(i.question)\n",
    "    for j in i.MultipleChoices:\n",
    "        print(j.choice_ans)\n",
    "    print(\"Correct Answer: \" , int(i.correct_ans) + 1 , \":\" , i.MultipleChoices[int(i.correct_ans)].choice_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open( 'objects.pkl' , 'wb') as file:\n",
    "    pickle.dump(res, file , pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = chain.invoke({'query' : question_query , 'subject' : 'science' , 'level' :'Grade 10'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the chemical formula for water?\n",
      "H2O\n",
      "CO2\n",
      "NaCl\n",
      "C6H12O6\n",
      "Correct Answer:  1 : H2O\n",
      "Which gas is responsible for the greenhouse effect?\n",
      "Oxygen\n",
      "Carbon dioxide\n",
      "Nitrogen\n",
      "Hydrogen\n",
      "Correct Answer:  2 : Carbon dioxide\n",
      "What is the unit of electric current?\n",
      "Volt\n",
      "Watt\n",
      "Ampere\n",
      "Ohm\n",
      "Correct Answer:  3 : Ampere\n"
     ]
    }
   ],
   "source": [
    "for i in res2.Questions:\n",
    "    print(i.question)\n",
    "    for j in i.MultipleChoices:\n",
    "        print(j.choice_ans)\n",
    "    print(\"Correct Answer: \" , int(i.correct_ans) + 1 , \":\" , i.MultipleChoices[int(i.correct_ans)].choice_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The stars twinkled in the night sky',\n",
       " 'The wind whispered through the trees',\n",
       " 'She sells seashells by the seashore',\n",
       " 'The waves crashed angrily against the shore']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1 = res.Questions[1]\n",
    "\n",
    "MCs = []\n",
    "\n",
    "for k , v in enumerate(Q1.MultipleChoices):\n",
    "        MCs.append(v.choice_ans)\n",
    "\n",
    "MCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'Watt' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/altaf/Projects/LLM/LangChain/L2-StudentTest.ipynb Cell 11\u001b[0m in \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/altaf/Projects/LLM/LangChain/L2-StudentTest.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m selected \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mWatt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/altaf/Projects/LLM/LangChain/L2-StudentTest.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m MCs\u001b[39m.\u001b[39;49mindex(selected)\n",
      "\u001b[0;31mValueError\u001b[0m: 'Watt' is not in list"
     ]
    }
   ],
   "source": [
    "selected = 'Watt'\n",
    "\n",
    "MCs.index(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['x' , 'a'] in [\"a\" , \"b\" , \"c\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
